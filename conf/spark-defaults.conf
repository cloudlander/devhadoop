#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

spark.driver.extraJavaOptions     -Dhadoop.log.dir=/Users/xzhang/cdp/logs -Dhadoop.log.file=spark_driver.log -Dlog4j.debug1 -Dhadoop.root.logger=DEBUG,RFA
#spark.driver.extraJavaOptions     -Dhadoop.log.dir=/Users/xzhang/cdp/logs -Dhadoop.log.file=spark_driver.log -Dlog4j.debug1 -Dhadoop.root.logger=DEBUG,RFA -agentlib:jdwp=transport=dt_socket,server=y,address=localhost:6000,suspend=n
#spark.executor.extraJavaOptions  -agentlib:jdwp=transport=dt_socket,server=y,address=localhost:6000,suspend=y

spark.sql.catalogImplementation=hive

spark.hadoop.fs.s3a.committer.name=directory
spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter

#already present in hive-site.xml
#spark.hadoop.hive.metastore.uris=thrift://localhost:9083
#spark.hadoop.hive.zookeeper.quorum=localhost:2181

#direct read mode
#spark.sql.extensions=com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension
spark.sql.extensions=com.hortonworks.spark.sql.rule.Extensions
spark.kryo.registrator=com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator

#jdbc mode
spark.sql.hive.hiveserver2.jdbc.url=jdbc:hive2://localhost:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
#spark.sql.hive.hiveserver2.jdbc.url.principal=hive/_HOST@SUPPORT.COM

#deprecated since CDP7.1.5
spark.sql.hive.hwc.execution.mode=spark
spark.datasource.hive.warehouse.read.jdbc.mode=cluster
spark.datasource.hive.warehouse.read.via.llap=false
#spark.datasource.hive.warehouse.metastoreUri=thrift://localhost:9083

# https://docs.cloudera.com/cdp-private-cloud-base/7.1.6/integrating-hive-and-bi/topics/hive-hwc-reader-mode.html
#spark.datasource.hive.warehouse.read.mode=DIRECT_READER_V1
#spark.datasource.hive.warehouse.read.mode=JDBC_CLUSTER
#JDBC_CLIENT does not work
#spark.datasource.hive.warehouse.read.mode=JDBC_CLIENT

# no needed
#spark.sql.htl.check=true
#spark.sql.htl.sparkCapabilities=CONNECTORREAD,HIVEFULLACIDREAD,HIVEFULLACIDWRITE,HIVEMANAGESTATS,HIVECACHEINVALIDATE,CONNECTORWRITE,SPARKSQL,EXTREAD,EXTWRITE,HIVESQL,HIVEBUCKET2

spark.yarn.jars=local:/Users/xzhang/cdp/spark/assembly/target/scala-2.11/jars/*
spark.jars=local:/Users/xzhang/cdp/hive-warehouse-connector/target/scala-2.11/hive-warehouse-connector-assembly-1.0.0.7.2.8.0-SNAPSHOT.jar

spark.driver.extraLibraryPath /Users/xzhang/cdp/hadoop-7.1.7/lib/native
spark.executor.extraLibraryPath /Users/xzhang/cdp/hadoop-7.1.7/lib/native 
spark.yarn.am.extraLibraryPath /Users/xzhang/cdp/hadoop-7.1.7/lib/native

spark.shuffle.registration.timeout  7200
spark.network.timeout            7200
spark.master                     yarn
spark.deployMode                 client
spark.eventLog.enabled           true
spark.eventLog.dir               file:///Users/xzhang/spark_history
spark.history.fs.logDirectory    file:///Users/xzhang/spark_history
spark.shuffle.service.enabled	 true
spark.shuffle.service.port       7337
spark.authenticate               true
spark.authenticate.enableSaslEncryption false
spark.network.crypto.enabled     true
